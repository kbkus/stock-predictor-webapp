{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "stock_env",
   "display_name": "stock_env",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # for web scraping\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "\n",
    "# sentiment analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "#from sentiment_dictionary import NEW_WORDS\n",
    "\n",
    "# install yahoo finance\n",
    "import yfinance as yf\n",
    "\n",
    "from config import save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_finviz(companies):\n",
    "    \"\"\"A function to scrape the finviz website.\n",
    "    \n",
    "    It will return the information as a data frame with columns for:\n",
    "    company, date, time, headline.\n",
    "    \"\"\"\n",
    "    \n",
    "    news_tables = {} # dictionary to store the news tables from each company\n",
    "    # url for the news website\n",
    "    finwiz_url = 'https://finviz.com/quote.ashx?t='\n",
    "    \n",
    "    print('{} companies to gather data for'.format(len(companies)))\n",
    "    count = 1\n",
    "    \n",
    "    # loop through each company you want to store data for\n",
    "    for company in companies:\n",
    "        print(count)\n",
    "        try:\n",
    "            url = finwiz_url + company\n",
    "            response = requests.get(url, headers={'user-agent': 'my-app/0.0.1'})\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            # find 'news-table' in soup and load it into 'news_tables'\n",
    "            news_table = soup.find(id='news-table')\n",
    "            # add the table to dictionary\n",
    "            news_tables[company] = news_table\n",
    "            count+=1\n",
    "        except:\n",
    "            print('error with {}'.format(company))\n",
    "            count +=1\n",
    "            continue\n",
    "        \n",
    "    parsed_news = []\n",
    "    # iterate through the news\n",
    "    # iterate through the news\n",
    "    for file_name, news_table in news_tables.items():\n",
    "        try:\n",
    "            # iterate through all tr tags in 'news_table'\n",
    "            for article in news_table.find_all('tr'):\n",
    "                # read the text from each tr tag into text\n",
    "                # get text from a only\n",
    "                text = article.a.get_text()\n",
    "                # splice text in the td tag into a list\n",
    "                date_scrape = article.td.text.split()\n",
    "                # if the length of the 'date_scrape' is 1, load 'time' as the only element\n",
    "\n",
    "                if len(date_scrape) == 1:\n",
    "                    time = date_scrape[0]\n",
    "\n",
    "                # else load 'date as the 1st element and 'time' as the second\n",
    "                else:\n",
    "                    date = date_scrape[0]\n",
    "                    time = date_scrape[1]\n",
    "                # extract the ticker from the file name, get the string up to the first '_'\n",
    "                company = file_name.split('_')[0]\n",
    "\n",
    "                # append ticker, date, time, and headline as a list to the parsed news list\n",
    "                parsed_news.append([company, date, time, text])\n",
    "        except:\n",
    "            print('error with {}'.format(file_name.split('_')[0]))\n",
    "            continue\n",
    "            \n",
    "    # convert parsed news list into a dataframe \n",
    "    columns = ['company','date','time','headline']\n",
    "    parsed_news = pd.DataFrame(parsed_news, columns = columns)\n",
    "    \n",
    "    # delete news from Saturday and Sunday\n",
    "    index_names = parsed_news[pd.to_datetime(parsed_news['date']).dt.weekday >=5].index\n",
    "    parsed_news.drop(index_names, inplace = True)\n",
    "    \n",
    "    # reset index\n",
    "    parsed_news = parsed_news.reset_index(drop=True)\n",
    "    \n",
    "    return parsed_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 companies to gather data for\n1\nerror with AAPL\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f1ac6e278800>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscrape_finviz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AAPL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-13cfe10969aa>\u001b[0m in \u001b[0;36mscrape_finviz\u001b[0;34m(companies)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# convert parsed news list into a dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'company'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'headline'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mparsed_news\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_news\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# delete news from Saturday and Sunday\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "scrape_finviz(['AAPL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}